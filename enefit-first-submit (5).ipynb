{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":151409076,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index -U --find-links=/kaggle/input/deeptables-dependecies deeptables==0.2.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install statsmodels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport math\nimport numpy as np \nimport pandas as pd \nimport polars as pl \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport holidays\nimport lightgbm as lgb\nimport tensorflow as tf, deeptables as dt\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow_addons.optimizers import AdamW\nfrom tensorflow.python.keras import backend as K\nfrom deeptables.models import DeepTable, ModelConfig\nfrom deeptables.models import deepnets\nimport joblib\n\nprint('Tensorflow version:', tf.__version__)\nprint('DeepTables version:', dt.__version__)\n\n# large number of warnings in data processing step\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# fast ai libraries\nfrom fastai.tabular.all import *\n\n# constants\nSEED = 2024 # global seed for notebook\nBATCH_SIZE = 1024\nEPOCHS = 20\n\n#library from Yelim\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 42\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\nseed_everything(seed=seed)\n\ndef clean_memory():\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\nclean_memory()\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport math\nimport numpy as np \nimport pandas as pd \nimport polars as pl \nimport matplotlib.pyplot as plt \nimport holidays\n\n\nfrom datetime import timedelta\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n#library from Yelim\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:25.360566Z","iopub.execute_input":"2024-01-27T06:38:25.360893Z","iopub.status.idle":"2024-01-27T06:38:26.903366Z","shell.execute_reply.started":"2024-01-27T06:38:25.360862Z","shell.execute_reply":"2024-01-27T06:38:26.902612Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class DataStorage:\n    \"\"\"\n    This class was copied out from:\n    https://www.kaggle.com/code/vitalykudelya/enefit-object-oriented-gbdt\n    \"\"\"\n    \n    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n\n    data_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n        \"row_id\",\n        \"prediction_unit_id\",\n    ]\n    client_cols = [\n        \"product_type\",\n        \"county\",\n        \"eic_count\",\n        \"installed_capacity\",\n        \"is_business\",\n        \"date\",\n    ]\n    gas_prices_cols = [\"forecast_date\", \"lowest_price_per_mwh\", \"highest_price_per_mwh\"]\n    electricity_prices_cols = [\"forecast_date\", \"euros_per_mwh\"]\n    forecast_weather_cols = [\n        \"latitude\",\n        \"longitude\",\n        \"hours_ahead\",\n        \"temperature\",\n        \"dewpoint\",\n        \"cloudcover_high\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_total\",\n        \"10_metre_u_wind_component\",\n        \"10_metre_v_wind_component\",\n        \"forecast_datetime\",\n        \"direct_solar_radiation\",\n        \"surface_solar_radiation_downwards\",\n        \"snowfall\",\n        \"total_precipitation\",\n    ]\n    historical_weather_cols = [\n        \"datetime\",\n        \"temperature\",\n        \"dewpoint\",\n        \"rain\",\n        \"snowfall\",\n        \"surface_pressure\",\n        \"cloudcover_total\",\n        \"cloudcover_low\",\n        \"cloudcover_mid\",\n        \"cloudcover_high\",\n        \"windspeed_10m\",\n        \"winddirection_10m\",\n        \"shortwave_radiation\",\n        \"direct_solar_radiation\",\n        \"diffuse_radiation\",\n        \"latitude\",\n        \"longitude\",\n    ]\n    location_cols = [\"longitude\", \"latitude\", \"county\"]\n    target_cols = [\n        \"target\",\n        \"county\",\n        \"is_business\",\n        \"product_type\",\n        \"is_consumption\",\n        \"datetime\",\n    ]\n\n    def __init__(self):\n        self.df_data = pl.read_csv(\n            os.path.join(self.root, \"train.csv\"),\n            columns=self.data_cols,\n            try_parse_dates=True,\n        )\n        self.df_client = pl.read_csv(\n            os.path.join(self.root, \"client.csv\"),\n            columns=self.client_cols,\n            try_parse_dates=True,\n        )\n        self.df_gas_prices = pl.read_csv(\n            os.path.join(self.root, \"gas_prices.csv\"),\n            columns=self.gas_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_electricity_prices = pl.read_csv(\n            os.path.join(self.root, \"electricity_prices.csv\"),\n            columns=self.electricity_prices_cols,\n            try_parse_dates=True,\n        )\n        self.df_electricity_prices = self.df_electricity_prices.with_columns(\n            self.df_electricity_prices['euros_per_mwh'].abs().alias('euros_per_mwh')\n        )\n        self.df_forecast_weather = pl.read_csv(\n            os.path.join(self.root, \"forecast_weather.csv\"),\n            columns=self.forecast_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_historical_weather = pl.read_csv(\n            os.path.join(self.root, \"historical_weather.csv\"),\n            columns=self.historical_weather_cols,\n            try_parse_dates=True,\n        )\n        self.df_weather_station_to_county_mapping = pl.read_csv(\n            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n            columns=self.location_cols,\n            try_parse_dates=True,\n        )\n        self.df_data = self.df_data.filter(\n            pl.col(\"datetime\") >= pd.to_datetime(\"2022-01-01\")\n        )\n        self.df_target = self.df_data.select(self.target_cols)\n\n        self.schema_data = self.df_data.schema\n        self.schema_client = self.df_client.schema\n        self.schema_gas_prices = self.df_gas_prices.schema\n        self.schema_electricity_prices = self.df_electricity_prices.schema\n        self.schema_forecast_weather = self.df_forecast_weather.schema\n        self.schema_historical_weather = self.df_historical_weather.schema\n        self.schema_target = self.df_target.schema\n\n        self.df_weather_station_to_county_mapping = (\n            self.df_weather_station_to_county_mapping.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n        )\n        \n    def run(self): # roll -1hr, for only T & T_d\n        #self.df_data = self.fill_target(self.df_data)\n        self.df_forecast_weather = self.fill_radiation(self.df_forecast_weather)\n        self.df_forecast_weather = self.fill_summertime(self.df_forecast_weather)\n        self.df_forecast_weather = self.separateTP(self.df_forecast_weather)\n        self.df_forecast_weather = self.expForecastHr(self.df_forecast_weather)\n        self.df_forecast_weather = self.snow2water(self.df_forecast_weather)\n        self.df_historical_weather = self.histRoll(self.df_historical_weather)\n        \n        #return self.df\n    \n    def run_test(self): # roll -1hr, for only T & T_d\n        #self.df_data = self.fill_target(self.df_data)\n        self.df_forecast_weather = self.separateTP(self.df_forecast_weather)\n        self.df_forecast_weather = self.expForecastHr(self.df_forecast_weather)\n        self.df_forecast_weather = self.snow2water(self.df_forecast_weather)\n        self.df_historical_weather = self.histRoll(self.df_historical_weather)\n        \n        #return self.df\n    \n        \n#     def fill_target(self, df):\n#         def _interpolate_group(group):\n#             group['target'] = group['target'].interpolate(method='linear')\n#             return group\n#         return pl.DataFrame(df.to_pandas().groupby(['prediction_unit_id', 'is_consumption']).apply(_interpolate_group))\n\n    def fill_radiation(self, df):\n        rad = df.to_pandas()['surface_solar_radiation_downwards'].values\n        idx = df['surface_solar_radiation_downwards'].is_null().to_numpy().nonzero()[0]\n        for i, ind in enumerate(idx):\n            tmp = df[idx][i]\n            df_b1 = df.filter(\n                pl.col('latitude') == tmp['latitude'], \n                pl.col('longitude') == tmp['longitude'],\n                abs(pl.col('forecast_datetime') - tmp['forecast_datetime']) < timedelta(days=2),\n                pl.col('forecast_datetime').dt.hour() == tmp['forecast_datetime'].dt.hour(),\n                pl.col('hours_ahead') == tmp['hours_ahead'])\n\n            # 결측값 검사 및 안전한 나눗셈\n            if df_b1['direct_solar_radiation'][0] is not None and df_b1['surface_solar_radiation_downwards'][0] not in [None, 0] \\\n               and df_b1['direct_solar_radiation'][2] is not None and df_b1['surface_solar_radiation_downwards'][2] not in [None, 0]:\n                fillValue = df_b1['direct_solar_radiation'][1] / ((np.divide(df_b1['direct_solar_radiation'][0], df_b1['surface_solar_radiation_downwards'][0]) +\n                    np.divide(df_b1['direct_solar_radiation'][2], df_b1['surface_solar_radiation_downwards'][2]))/2)\n                rad[ind] = fillValue\n            else:\n                rad[ind] = 0  # 결측값 대체를 위한 기본값 설정\n\n        df.replace('surface_solar_radiation_downwards', pl.Series(rad))\n        return df\n\n\n    def fill_summertime(self, df):\n        missingDate = list(set(pd.date_range('2021-09-01', '2023-06-02', freq='h')[3:-22]) - set(df.to_pandas()['forecast_datetime'].unique()))\n        hrs_ahead = 2\n        add_df = pd.DataFrame()\n        for date in missingDate:\n            tmp = df.filter(abs(pl.col('forecast_datetime') - date) < timedelta(hours=2),\n                            pl.col('hours_ahead') <= 2).sort('latitude', 'longitude').to_pandas()\n            values_1 = None  # 초기화\n            for _, row in tmp.iterrows():\n                if row['hours_ahead'] == 1:\n                    values_1 = row\n                elif row['hours_ahead'] == 2 and values_1 is not None:\n                    values_2 = row\n                    average_values = pd.Series([(v1+v2)/2 if c != 'forecast_datetime' else date for (v1,v2,c) in zip(values_1,values_2,values_2.keys())],\n                                            index=values_2.keys())\n                    average_values['hours_ahead'] = hrs_ahead\n                    add_df = pd.concat([add_df, average_values.to_frame().T]).reset_index(drop=True)\n        return pl.DataFrame(pd.concat([df.to_pandas(), add_df]).reset_index(drop=True))\n\n\n    def snow2water(self, df): # for historical: [cm]/7->[mm]\n        return df.with_columns(\n            (df['snowfall']/7).alias('snowfall_mm'))\n\n    def separateTP(self, df): \n    # Adjust the indentation as needed to match the rest of your class\n        df = df.with_columns([\n            (df['total_precipitation'] - df['snowfall'] / 100).alias('rain')\n        ])\n        return df\n\n    def expForecastHr(self, df):\n    # Ensure the indentation here matches the rest of your class\n        def _exp(x):\n            return np.exp(x) / np.exp(48)\n\n        df = df.with_columns([\n            df['hours_ahead'].apply(_exp).alias('exp_hours_ahead')\n        ])\n        return df\n    \n    def histRoll(self, df): # roll -1hr, for only T & T_d\n        df = df.with_columns([\n            df['temperature'].shift(-1).alias('temperature')\n        ])\n        df = df.with_columns([\n            df['dewpoint'].shift(-1).alias('dewpoint')\n        ])\n        return df\n\n    def update_with_new_data(\n        self,\n        df_new_client,\n        df_new_gas_prices,\n        df_new_electricity_prices,\n        df_new_forecast_weather,\n        df_new_historical_weather,\n        df_new_target,\n    ):\n        df_new_client = pl.from_pandas(\n            df_new_client[self.client_cols], schema_overrides=self.schema_client\n        )\n        df_new_gas_prices = pl.from_pandas(\n            df_new_gas_prices[self.gas_prices_cols],\n            schema_overrides=self.schema_gas_prices,\n        )\n        df_new_electricity_prices = pl.from_pandas(\n            df_new_electricity_prices[self.electricity_prices_cols],\n            schema_overrides=self.schema_electricity_prices,\n        )\n        df_new_forecast_weather = pl.from_pandas(\n            df_new_forecast_weather[self.forecast_weather_cols],\n            schema_overrides=self.schema_forecast_weather,\n        )\n        df_new_historical_weather = pl.from_pandas(\n            df_new_historical_weather[self.historical_weather_cols],\n            schema_overrides=self.schema_historical_weather,\n        )\n        df_new_target = pl.from_pandas(\n            df_new_target[self.target_cols], schema_overrides=self.schema_target\n        )\n\n        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n            [\"date\", \"county\", \"is_business\", \"product_type\"]\n        )\n        self.df_gas_prices = pl.concat([self.df_gas_prices, df_new_gas_prices]).unique(\n            [\"forecast_date\"]\n        )\n        self.df_electricity_prices = pl.concat(\n            [self.df_electricity_prices, df_new_electricity_prices]\n        ).unique([\"forecast_date\"])\n        self.df_forecast_weather = pl.concat(\n            [self.df_forecast_weather, df_new_forecast_weather]\n        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n        self.df_historical_weather = pl.concat(\n            [self.df_historical_weather, df_new_historical_weather]\n        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n        )\n\n    def preprocess_test(self, df_test):\n        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n        df_test = pl.from_pandas(\n            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n        )\n        return df_test","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:26.905258Z","iopub.execute_input":"2024-01-27T06:38:26.905675Z","iopub.status.idle":"2024-01-27T06:38:26.937253Z","shell.execute_reply.started":"2024-01-27T06:38:26.905641Z","shell.execute_reply":"2024-01-27T06:38:26.936600Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class FeaturesGenerator:\n    \"\"\"\n    This class was copied out from:\n    https://www.kaggle.com/code/vitalykudelya/enefit-object-oriented-gbdt\n    \"\"\"\n    def __init__(self, data_storage):\n        self.data_storage = data_storage\n\n    def _add_general_features(self, df_features):\n        df_features = (\n            df_features.with_columns(\n                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                pl.col(\"datetime\").dt.day().alias(\"day\"),\n                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n                pl.col(\"datetime\").dt.month().alias(\"month\"),\n                pl.col(\"datetime\").dt.year().alias(\"year\"),\n            )\n            .with_columns(\n                pl.concat_str(\n                    \"county\",\n                    \"is_business\",\n                    \"product_type\",\n                    \"is_consumption\",\n                    separator=\"_\",\n                ).alias(\"segment\"),\n            )\n            .with_columns(\n                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n            )\n        )\n        return df_features\n\n    def _add_client_features(self, df_features):\n        df_client = self.data_storage.df_client\n\n        df_features = df_features.join(\n            df_client.with_columns(\n                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n            ),\n            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n            how=\"left\",\n        )\n        return df_features\n\n    def _add_forecast_weather_features(self, df_features):\n        df_forecast_weather = self.data_storage.df_forecast_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_forecast_weather = (\n            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n            #.filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n            .drop(\"hours_ahead\")\n            .with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_forecast_weather_date = (\n            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_forecast_weather_local = (\n            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [0, 7 * 24]:\n            df_features = df_features.join(\n                df_forecast_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_forecast_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_forecast_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_forecast_local_{hours_lag}h\",\n            )\n\n        return df_features\n\n    def _add_historical_weather_features(self, df_features):\n        df_historical_weather = self.data_storage.df_historical_weather\n        df_weather_station_to_county_mapping = (\n            self.data_storage.df_weather_station_to_county_mapping\n        )\n\n        df_historical_weather = (\n            df_historical_weather.with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n            )\n            .join(\n                df_weather_station_to_county_mapping,\n                how=\"left\",\n                on=[\"longitude\", \"latitude\"],\n            )\n            .drop(\"longitude\", \"latitude\")\n        )\n\n        df_historical_weather_date = (\n            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n        )\n\n        df_historical_weather_local = (\n            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n            .group_by(\"county\", \"datetime\")\n            .mean()\n        )\n\n        for hours_lag in [2 * 24, 7 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n            df_features = df_features.join(\n                df_historical_weather_local.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n                ),\n                on=[\"county\", \"datetime\"],\n                how=\"left\",\n                suffix=f\"_historical_local_{hours_lag}h\",\n            )\n\n        for hours_lag in [1 * 24]:\n            df_features = df_features.join(\n                df_historical_weather_date.with_columns(\n                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n                )\n                #.filter(pl.col(\"hour\") <= 10)\n                .drop(\"hour\"),\n                on=\"datetime\",\n                how=\"left\",\n                suffix=f\"_historical_{hours_lag}h\",\n            )\n\n        return df_features\n\n#     def _add_target_features(self, df_features):\n#         df_target = self.data_storage.df_target\n\n#         df_target_all_type_sum = (\n#             df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n#             .sum()\n#             .drop(\"product_type\")\n#         )\n\n#         df_target_all_county_type_sum = (\n#             df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n#             .sum()\n#             .drop(\"product_type\", \"county\")\n#         )\n\n#         for hours_lag in [\n#             2 * 24,\n#             3 * 24,\n#             4 * 24,\n#             5 * 24,\n#             6 * 24,\n#             7 * 24,\n#             8 * 24,\n#             9 * 24,\n#             10 * 24,\n#             11 * 24,\n#             12 * 24,\n#             13 * 24,\n#             14 * 24,\n#             6,      ###\n#             12,     ###\n#             84,     ###\n#             3096,   ### add juwon\n#         ]:\n#             df_features = df_features.join(\n#                 df_target.with_columns(\n#                     pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n#                 ).rename({\"target\": f\"target_{hours_lag}h\"}),\n#                 on=[\n#                     \"county\",\n#                     \"is_business\",\n#                     \"product_type\",\n#                     \"is_consumption\",\n#                     \"datetime\",\n#                 ],\n#                 how=\"left\",\n#             )\n\n#         for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n#             df_features = df_features.join(\n#                 df_target_all_type_sum.with_columns(\n#                     pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n#                 ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n#                 on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n#                 how=\"left\",\n#             )\n\n#             df_features = df_features.join(\n#                 df_target_all_county_type_sum.with_columns(\n#                     pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n#                 ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n#                 on=[\"is_business\", \"is_consumption\", \"datetime\"],\n#                 how=\"left\",\n#                 suffix=f\"_all_county_type_sum_{hours_lag}h\",\n#             )\n\n#         cols_for_stats = [\n#             f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n#         ]\n#         df_features = df_features.with_columns(\n#             df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n#             df_features.select(cols_for_stats)\n#             .transpose()\n#             .std()\n#             .transpose()\n#             .to_series()\n#             .alias(f\"target_std\"),\n#         )\n\n#         for target_prefix, lag_nominator, lag_denomonator in [\n#             (\"target\", 24 * 7, 24 * 14),\n#             (\"target\", 24 * 2, 24 * 9),\n#             (\"target\", 24 * 3, 24 * 10),\n#             (\"target\", 24 * 2, 24 * 3),\n#             (\"target_all_type_sum\", 24 * 2, 24 * 3),\n#             (\"target_all_type_sum\", 24 * 7, 24 * 14),\n#             (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n#             (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n#         ]:\n#             df_features = df_features.with_columns(\n#                 (\n#                     pl.col(f\"{target_prefix}_{lag_nominator}h\")\n#                     / (pl.col(f\"{target_prefix}_{lag_denomonator}h\") + 1e-3)\n#                 ).alias(f\"{target_prefix}_ratio_{lag_nominator}_{lag_denomonator}\")\n#             )\n\n#         return df_features\n\n    def _reduce_memory_usage(self, df_features):\n        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n        return df_features\n\n    def _drop_columns(self, df_features):\n        df_features = df_features.drop(\n           \"datetime\", \"hour\", \"dayofyear\"\n        )\n        return df_features\n\n    def _to_pandas(self, df_features, y):\n        cat_cols = [\n            \"county\",\n            \"is_business\",\n            \"product_type\",\n            \"is_consumption\",\n            \"segment\",\n        ]\n\n        if y is not None:\n            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n        else:\n            df_features = df_features.to_pandas()\n\n        df_features = df_features.set_index(\"row_id\")\n        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n\n        return df_features\n\n    def generate_features(self, df_prediction_items):\n        if \"target\" in df_prediction_items.columns:\n            df_prediction_items, y = (\n                df_prediction_items.drop(\"target\"),\n                df_prediction_items.select(\"target\"),\n            )\n        else:\n            y = None\n\n        df_features = df_prediction_items.with_columns(\n            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n        )\n\n        for add_features in [\n            self._add_general_features,\n            self._add_client_features,\n            self._add_forecast_weather_features,\n            self._add_historical_weather_features,\n            #self._add_target_features,\n            self._reduce_memory_usage,\n            self._drop_columns,\n        ]:\n            df_features = add_features(df_features)\n\n        df_features = self._to_pandas(df_features, y)\n\n        return df_features","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:26.938144Z","iopub.execute_input":"2024-01-27T06:38:26.938367Z","iopub.status.idle":"2024-01-27T06:38:26.961792Z","shell.execute_reply.started":"2024-01-27T06:38:26.938343Z","shell.execute_reply":"2024-01-27T06:38:26.961025Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def convert_2_dataloader(df, _seed):\n\n    # define categorical and continous numerical feature column names (on small number of features)\n    # from train.csv\n    cat_names = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", \"segment\"]\n    # from datetime column\n    cat_names += [\"weekday\", \"month\", 'sin(dayofyear)', 'cos(dayofyear)', 'sin(hour)', 'cos(hour)']\n    # from https://www.kaggle.com/code/albansteff/enefit-estonian-holidays-lb-65-79 notebook\n    cat_names += ['country_holiday']\n    \n    # from client.csv\n    cont_names = [\"installed_capacity\", \"eic_count\"]\n    \n    # from forecast_weather.csv (next 0 hours)\n    cont_names += [_ for _ in df.columns if \"_forecast_0h\" in _]\n    cont_names += [_ for _ in df.columns if \"_forecast_local_0h\" in _]\n    # from forecast_weather.csv (next 24 hours)\n    cont_names += [_ for _ in df.columns if \"_forecast_24h\" in _]\n    cont_names += [_ for _ in df.columns if \"_forecast_local_24h\" in _]\n    \n    # from historical_weather.csv (last 24/48 hours)\n    cont_names += [_ for _ in df.columns if \"_historical_24h\" in _]\n    cont_names += [_ for _ in df.columns if \"_historical_48h\" in _]\n    cont_names += [_ for _ in df.columns if \"_historical_local_48h\" in _]\n    \n    # add all historical target values (last n hours)\n    cont_names += df.filter(regex=(\"target_.[0-9]*h\")).columns.tolist()\n    cont_names += ['target_mean', 'target_std']\n    \n    # added aggregated target values\n    cont_names += [_ for _ in df.columns if \"target_all_\" in _]\n    \n    # add ratios between last kown target values\n    cont_names += df.filter(regex=(\"target_ratio_.[0-9]\")).columns.tolist()\n    \n    procs = [Categorify, FillMissing, Normalize]\n    \n    # log transform target variable\n    df.loc[:, 'target'] = np.log1p(df['target'])\n        \n    # convert pandas DataFrame to fastai DataLoader object\n    # code snippet taken from\n    # https://docs.fast.ai/tabular.learner.html\n    splits = RandomSplitter(valid_pct=0.2, seed = _seed)(df)\n    \n    # tabular object (only categorical features)\n    to = TabularPandas(df[cat_names + cont_names + [\"target\"]],\n                       procs = procs,\n                       cat_names = cat_names,\n                       cont_names = cont_names,\n                       y_names = [\"target\"],\n                       splits=splits)\n    # create dataloader\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    dls = to.dataloaders(BATCH_SIZE, device = device)\n     \n    # return all dataloaders as tuple\n    return dls\n\ndef add_custom_features(df):\n    \"\"\"\n    Function inspired by notebook:\n    https://www.kaggle.com/code/albansteff/enefit-estonian-holidays-lb-65-79\n    \"\"\"\n    \n    # code bellow same as in NB v17 add_holidays_as_binary_features function\n    estonian_holidays = holidays.country_holidays('EE', years=range(2021, 2026))\n    estonian_holidays = [pd.to_datetime(_) for _ in estonian_holidays.keys()]\n    \n    df['country_holiday'] = df['date'].isin(estonian_holidays) * 1\n    del df['date']\n    \n    # log transform histrocial target values\n    _cols = df.filter(regex=(\"target_.[0-9]*h\")).columns.tolist()\n    for _col in _cols:\n        df.loc[:, _col] = np.log1p(df[_col])\n    \n    # log transform aggregated target values\n    _cols = [_ for _ in df.columns if \"target_all_\" in _]\n    for _col in _cols:\n        df.loc[:, _col] = np.log1p(df[_col])  \n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:26.962679Z","iopub.execute_input":"2024-01-27T06:38:26.962924Z","iopub.status.idle":"2024-01-27T06:38:26.977996Z","shell.execute_reply.started":"2024-01-27T06:38:26.962891Z","shell.execute_reply":"2024-01-27T06:38:26.977287Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class DataTransformer:\n    def __init__(self, df):\n        self.df = df\n\n    def transform(self):\n        self.add_season()\n        self.add_daypart_with_sin_hour()\n        self.add_feels_like_temperature()\n        #self.add_energy_usage_trend()\n        self.add_temp_change()\n        self.add_prec_change()\n        self.add_autocorr_features()\n        self.add_energy_price_volatility_and_trend()\n        #self.perform_clustering()\n        self.analyze_transit_and_charging_access()\n        return self.df\n\n    def add_season(self):\n        def get_season(month):\n            if month in [3, 4, 5]:\n                return 1 #spring\n            elif month in [6, 7, 8]:\n                return 2 #summer\n            elif month in [9, 10, 11]:\n                return 3 #fall\n            else:\n                return 4 #winter\n        \n        self.df['season'] = self.df['month'].apply(get_season)\n\n    def add_daypart_with_sin_hour(self):\n        def get_daypart(sin_hour):\n            if sin_hour > 0:\n                return 1  # sin(hour) 양수: 오전~오후\n            else:\n                return 2      # sin(hour) 음수: 저녁~밤\n\n        self.df['daypart'] = self.df['sin(hour)'].apply(get_daypart)\n\n    def add_feels_like_temperature(self):\n        def calculate_feels_like(T, u, v):\n            wind_speed = (u**2 + v**2)**0.5\n            if wind_speed < 4.8:\n                return T\n            else:\n                return 13.12 + 0.6215 * T - 11.37 * (wind_speed ** 0.16) + 0.3965 * T * (wind_speed ** 0.16)\n\n        self.df['feels_like_temp'] = self.df.apply(lambda row: calculate_feels_like(row['temperature'], row['10_metre_u_wind_component'], row['10_metre_v_wind_component']), axis=1)\n\n\n#     def add_energy_usage_trend(self, period = 7):\n#         self.df['energy_trend'] = self.df['target'].rolling(window = period).mean()\n\n    def add_temp_change(self, interval = 24):\n        self.df['temp_change'] = self.df['temperature'].diff(periods = interval)\n\n    def add_prec_change(self):\n        self.df['precipitation_change'] = self.df['total_precipitation'].diff()\n\n    def add_autocorr_features(self, lags = 10):\n        acf_values = acf(self.df['target'], nlags = lags)\n        pacf_values = pacf(self.df['target'], nlags = lags)\n        for i in range(lags+1):\n            self.df[f'acf_lag_{i}'] = acf_values[i]\n            self.df[f'pacf_lag{i}'] = pacf_values[i]\n\n    def add_energy_price_volatility_and_trend(self, window = 7):\n        self.df['energy_price_volatility'] = self.df['target'].rolling(window = window).std()\n\n    # def perform_clustering(self, n_clusters = 3, features = None):\n    #     if features is None:\n    #         features = ['target_24h', 'target_48h', 'temperature', 'cloudcover_total']\n    #     scaler = StandardScaler()\n    #     scaled_data = scaler.fit_transform(self.df[features])\n\n    #     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    #     self.df['cluster'] = kmeans.fit_predict(scaled_data)\n\n    def analyze_transit_and_charging_access(self):\n\n        results = []\n        for consumption_status in [0, 1]:\n            subset = self.df[self.df['is_consumption'] == consumption_status]\n\n            # 대중교통 이용률 분석\n            business_hours_energy = subset[subset['is_business'] == 1]['target'].mean()\n            non_business_hours_energy = subset[subset['is_business'] == 0]['target'].mean()\n            transit_usage_estimate = business_hours_energy - non_business_hours_energy\n\n            # 전기차 충전소 접근성 분석\n            high_capacity_energy = subset[subset['installed_capacity'] > subset['installed_capacity'].median()]['target'].mean()\n            low_capacity_energy = subset[subset['installed_capacity'] <= subset['installed_capacity'].median()]['target'].mean()\n            charging_access_estimate = high_capacity_energy - low_capacity_energy\n\n            results.append((consumption_status, transit_usage_estimate, charging_access_estimate))\n\n        # 결과를 하나의 컬럼으로 합침\n        for consumption_status, transit_estimate, charging_estimate in results:\n            self.df[f'transit_usage_estimate_{consumption_status}'] = transit_estimate\n            self.df[f'charging_access_estimate_{consumption_status}'] = charging_estimate","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:26.979655Z","iopub.execute_input":"2024-01-27T06:38:26.979895Z","iopub.status.idle":"2024-01-27T06:38:26.993490Z","shell.execute_reply.started":"2024-01-27T06:38:26.979871Z","shell.execute_reply":"2024-01-27T06:38:26.992908Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import numpy as np #wind  \nimport pandas as pd\n\n#train_dataset data 변환 (weekday -> weekend, wind dir, speed -> U10, V10)\nclass TrainDataTransform:\n    def __init__(self, df):\n        self.df = df\n\n    def transform(self):\n        self.is_weekend()\n        self.wind_data_to_UV()\n        return self.df\n\n    #weekend 판별 함수\n    def is_weekend(self):\n        self.df['is_weekend'] = np.where(self.df['weekday'] > 4, 1, 0)\n        \n    def wind_data_to_UV(self):\n        self.df['U10'] = self.df['windspeed_10m'] * np.cos(np.radians(270 - self.df['winddirection_10m']))\n        self.df['V10'] = self.df['windspeed_10m'] * np.sin(np.radians(270 - self.df['winddirection_10m']))\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:26.994386Z","iopub.execute_input":"2024-01-27T06:38:26.994627Z","iopub.status.idle":"2024-01-27T06:38:27.005698Z","shell.execute_reply.started":"2024-01-27T06:38:26.994601Z","shell.execute_reply":"2024-01-27T06:38:27.004907Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data_storage = DataStorage()\ndata_storage.run()\nfeatures_generator = FeaturesGenerator(data_storage=data_storage)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:27.006559Z","iopub.execute_input":"2024-01-27T06:38:27.006805Z","iopub.status.idle":"2024-01-27T06:38:54.133211Z","shell.execute_reply.started":"2024-01-27T06:38:27.006779Z","shell.execute_reply":"2024-01-27T06:38:54.132415Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = features_generator.generate_features(data_storage.df_data)\n# exclude rows with missing target value\ntrain_dataset = train_dataset[train_dataset['target'].notnull()]\n# add estonian holidays\ntrain_dataset = add_custom_features(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:54.134848Z","iopub.execute_input":"2024-01-27T06:38:54.135122Z","iopub.status.idle":"2024-01-27T06:38:57.462744Z","shell.execute_reply.started":"2024-01-27T06:38:54.135079Z","shell.execute_reply":"2024-01-27T06:38:57.461981Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#이거 먼저 와야함, 예림 추가\ntrain_dataset['eic_count'] = train_dataset['eic_count'].fillna(method = 'bfill')\ntrain_dataset['installed_capacity'] = train_dataset['installed_capacity'].fillna(method = 'bfill')\n\n######################## 이부분에서 missing value확인하기?\n\n#add yelim\ntrain_dataset = DataTransformer(train_dataset)\ntrain_dataset = train_dataset.transform()\n\n#add joonyong\ntrain_dataset = TrainDataTransform(train_dataset)\ntrain_dataset = train_dataset.transform()\n\n\n### drop county == 12 , prec_change Nan,  yelim added ####\ntrain_dataset = train_dataset[train_dataset['county'] != 12]\ntrain_dataset['precipitation_change'] = train_dataset['precipitation_change'].fillna(method='bfill')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:38:57.464059Z","iopub.execute_input":"2024-01-27T06:38:57.464377Z","iopub.status.idle":"2024-01-27T06:39:36.179806Z","shell.execute_reply.started":"2024-01-27T06:38:57.464349Z","shell.execute_reply":"2024-01-27T06:39:36.178895Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:39:36.180894Z","iopub.execute_input":"2024-01-27T06:39:36.181415Z","iopub.status.idle":"2024-01-27T06:39:36.207422Z","shell.execute_reply.started":"2024-01-27T06:39:36.181380Z","shell.execute_reply":"2024-01-27T06:39:36.206661Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       county is_business product_type is_consumption  prediction_unit_id  \\\nrow_id                                                                      \n366048      0           0            1              0                   0   \n366049      0           0            1              1                   0   \n366050      0           0            2              0                   1   \n366051      0           0            2              1                   1   \n366052      0           0            3              0                   2   \n\n        day  weekday  month  year  segment  ...  acf_lag_10  pacf_lag10  \\\nrow_id                                      ...                           \n366048    1        6      1  2022  0_0_1_0  ...    0.005509    -0.01286   \n366049    1        6      1  2022  0_0_1_1  ...    0.005509    -0.01286   \n366050    1        6      1  2022  0_0_2_0  ...    0.005509    -0.01286   \n366051    1        6      1  2022  0_0_2_1  ...    0.005509    -0.01286   \n366052    1        6      1  2022  0_0_3_0  ...    0.005509    -0.01286   \n\n        energy_price_volatility  transit_usage_estimate_0  \\\nrow_id                                                      \n366048                      NaN                -23.837394   \n366049                      NaN                -23.837394   \n366050                      NaN                -23.837394   \n366051                      NaN                -23.837394   \n366052                      NaN                -23.837394   \n\n        charging_access_estimate_0  transit_usage_estimate_1  \\\nrow_id                                                         \n366048                  156.280232                606.576062   \n366049                  156.280232                606.576062   \n366050                  156.280232                606.576062   \n366051                  156.280232                606.576062   \n366052                  156.280232                606.576062   \n\n        charging_access_estimate_1  is_weekend       U10      V10  \nrow_id                                                             \n366048                  731.338898           1 -0.087235  4.58796  \n366049                  731.338898           1 -0.087235  4.58796  \n366050                  731.338898           1 -0.087235  4.58796  \n366051                  731.338898           1 -0.087235  4.58796  \n366052                  731.338898           1 -0.087235  4.58796  \n\n[5 rows x 183 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>county</th>\n      <th>is_business</th>\n      <th>product_type</th>\n      <th>is_consumption</th>\n      <th>prediction_unit_id</th>\n      <th>day</th>\n      <th>weekday</th>\n      <th>month</th>\n      <th>year</th>\n      <th>segment</th>\n      <th>...</th>\n      <th>acf_lag_10</th>\n      <th>pacf_lag10</th>\n      <th>energy_price_volatility</th>\n      <th>transit_usage_estimate_0</th>\n      <th>charging_access_estimate_0</th>\n      <th>transit_usage_estimate_1</th>\n      <th>charging_access_estimate_1</th>\n      <th>is_weekend</th>\n      <th>U10</th>\n      <th>V10</th>\n    </tr>\n    <tr>\n      <th>row_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>366048</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2022</td>\n      <td>0_0_1_0</td>\n      <td>...</td>\n      <td>0.005509</td>\n      <td>-0.01286</td>\n      <td>NaN</td>\n      <td>-23.837394</td>\n      <td>156.280232</td>\n      <td>606.576062</td>\n      <td>731.338898</td>\n      <td>1</td>\n      <td>-0.087235</td>\n      <td>4.58796</td>\n    </tr>\n    <tr>\n      <th>366049</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2022</td>\n      <td>0_0_1_1</td>\n      <td>...</td>\n      <td>0.005509</td>\n      <td>-0.01286</td>\n      <td>NaN</td>\n      <td>-23.837394</td>\n      <td>156.280232</td>\n      <td>606.576062</td>\n      <td>731.338898</td>\n      <td>1</td>\n      <td>-0.087235</td>\n      <td>4.58796</td>\n    </tr>\n    <tr>\n      <th>366050</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2022</td>\n      <td>0_0_2_0</td>\n      <td>...</td>\n      <td>0.005509</td>\n      <td>-0.01286</td>\n      <td>NaN</td>\n      <td>-23.837394</td>\n      <td>156.280232</td>\n      <td>606.576062</td>\n      <td>731.338898</td>\n      <td>1</td>\n      <td>-0.087235</td>\n      <td>4.58796</td>\n    </tr>\n    <tr>\n      <th>366051</th>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2022</td>\n      <td>0_0_2_1</td>\n      <td>...</td>\n      <td>0.005509</td>\n      <td>-0.01286</td>\n      <td>NaN</td>\n      <td>-23.837394</td>\n      <td>156.280232</td>\n      <td>606.576062</td>\n      <td>731.338898</td>\n      <td>1</td>\n      <td>-0.087235</td>\n      <td>4.58796</td>\n    </tr>\n    <tr>\n      <th>366052</th>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>2022</td>\n      <td>0_0_3_0</td>\n      <td>...</td>\n      <td>0.005509</td>\n      <td>-0.01286</td>\n      <td>NaN</td>\n      <td>-23.837394</td>\n      <td>156.280232</td>\n      <td>606.576062</td>\n      <td>731.338898</td>\n      <td>1</td>\n      <td>-0.087235</td>\n      <td>4.58796</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 183 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import enefit\nfrom sklearn.linear_model import LinearRegression\n\n# 모델 초기화\nsimple_model = LinearRegression()\n\ntrain_dataset = train_dataset.fillna(method='bfill')\ntrain_dataset = train_dataset.fillna(method='ffill')\n# 데이터 준비 및 모델 학습\n# df_train_features: 훈련 데이터프레임\nX_train = train_dataset.drop(columns=['target'])\ny_train = train_dataset['target']\nsimple_model.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:39:36.208443Z","iopub.execute_input":"2024-01-27T06:39:36.208729Z","iopub.status.idle":"2024-01-27T06:41:40.895093Z","shell.execute_reply.started":"2024-01-27T06:39:36.208698Z","shell.execute_reply":"2024-01-27T06:41:40.893800Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"LinearRegression()","text/html":"<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# enefit API를 사용한 테스트 데이터셋 처리\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:41:40.896268Z","iopub.execute_input":"2024-01-27T06:41:40.896551Z","iopub.status.idle":"2024-01-27T06:41:40.902177Z","shell.execute_reply.started":"2024-01-27T06:41:40.896522Z","shell.execute_reply":"2024-01-27T06:41:40.900988Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\n\nfor (\n    df_test, \n    df_new_target, \n    df_new_client, \n    df_new_historical_weather,\n    df_new_forecast_weather, \n    df_new_electricity_prices, \n    df_new_gas_prices, \n    df_sample_prediction\n) in iter_test:\n\n    # 데이터 전처리 및 특성 생성\n    data_storage.update_with_new_data(\n        df_new_client=df_new_client,\n        df_new_gas_prices=df_new_gas_prices,\n        df_new_electricity_prices=df_new_electricity_prices,\n        df_new_forecast_weather=df_new_forecast_weather,\n        df_new_historical_weather=df_new_historical_weather,\n        df_new_target=df_new_target\n    )\n    df_test = data_storage.preprocess_test(df_test)\n    data_storage.run()\n    df_test = data_storage.df\n    \n    df_test = features_generator.generate_features(df_test)\n    df_test = df_test[df_test['target'].notnull()]\n    df_test = add_custom_features(df_test)\n\n    #이거 먼저 와야함, 예림 추가\n    df_test['eic_count'] = df_test['eic_count'].fillna(method = 'bfill')\n    df_test['installed_capacity'] = df_test['installed_capacity'].fillna(method = 'bfill')\n    #add yelim\n    df_test = DataTransformer(df_test)\n    df_test = df_test.transform()\n    #add joonyong\n    df_test = TrainDataTransform(df_test)\n    df_test = df_test.transform()\n    ### drop county == 12 , prec_change Nan,  yelim added ####\n    df_test = df_test[df_test['county'] != 12]\n    df_test_features = df_test['precipitation_change'].fillna(method='bfill')\n    \n    prec = model.predict(df_test_features)\n    \n    \n    df_sample_prediction[\"target\"] = prec\n    env.predict(df_sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T06:41:40.903405Z","iopub.execute_input":"2024-01-27T06:41:40.903728Z","iopub.status.idle":"2024-01-27T06:41:41.579635Z","shell.execute_reply.started":"2024-01-27T06:41:40.903676Z","shell.execute_reply":"2024-01-27T06:41:41.578256Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mShapeError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     df_test, \n\u001b[1;32m      3\u001b[0m     df_new_target, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 데이터 전처리 및 특성 생성\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mdata_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_with_new_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_gas_prices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_gas_prices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_electricity_prices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_electricity_prices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_forecast_weather\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_forecast_weather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_historical_weather\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_historical_weather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_new_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_new_target\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     df_test \u001b[38;5;241m=\u001b[39m data_storage\u001b[38;5;241m.\u001b[39mpreprocess_test(df_test)\n\u001b[1;32m     22\u001b[0m     data_storage\u001b[38;5;241m.\u001b[39mrun()\n","Cell \u001b[0;32mIn[2], line 278\u001b[0m, in \u001b[0;36mDataStorage.update_with_new_data\u001b[0;34m(self, df_new_client, df_new_gas_prices, df_new_electricity_prices, df_new_forecast_weather, df_new_historical_weather, df_new_target)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_gas_prices \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_gas_prices, df_new_gas_prices])\u001b[38;5;241m.\u001b[39munique(\n\u001b[1;32m    273\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforecast_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    274\u001b[0m )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_electricity_prices \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m    276\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_electricity_prices, df_new_electricity_prices]\n\u001b[1;32m    277\u001b[0m )\u001b[38;5;241m.\u001b[39munique([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforecast_date\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_forecast_weather \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_forecast_weather\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_new_forecast_weather\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munique([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforecast_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhours_ahead\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_historical_weather \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m    282\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_historical_weather, df_new_historical_weather]\n\u001b[1;32m    283\u001b[0m )\u001b[38;5;241m.\u001b[39munique([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_target \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_target, df_new_target])\u001b[38;5;241m.\u001b[39munique(\n\u001b[1;32m    285\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounty\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_business\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_consumption\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    286\u001b[0m )\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polars/functions/eager.py:184\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(items, how, rechunk, parallel)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first, pl\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 184\u001b[0m         out \u001b[38;5;241m=\u001b[39m wrap_df(\u001b[43mplr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43melems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical_relaxed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         out \u001b[38;5;241m=\u001b[39m wrap_ldf(\n\u001b[1;32m    187\u001b[0m             plr\u001b[38;5;241m.\u001b[39mconcat_lf(\n\u001b[1;32m    188\u001b[0m                 [df\u001b[38;5;241m.\u001b[39mlazy() \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m elems],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m             )\n\u001b[1;32m    193\u001b[0m         )\u001b[38;5;241m.\u001b[39mcollect(no_optimization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mShapeError\u001b[0m: unable to append to a DataFrame of width 19 with a DataFrame of width 16"],"ename":"ShapeError","evalue":"unable to append to a DataFrame of width 19 with a DataFrame of width 16","output_type":"error"}]},{"cell_type":"code","source":"# 학습 데이터셋의 피처들\ntrain_features = set(train_dataset.columns)\n\n# 테스트 데이터셋의 피처들\ntest_features = set(df_test_features.columns)\n\n# 학습 데이터셋에만 있는 피처들\nonly_in_train = train_features - test_features\nprint(\"Only in train dataset:\", only_in_train)\n\n# 테스트 데이터셋에만 있는 피처들\nonly_in_test = test_features - train_features\nprint(\"Only in test dataset:\", only_in_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/forecast_weather.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"df_train_features = train_dataset[train_dataset['target'].notnull()]\ndf_train_features.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_START = 1e-7\nLR_MAX = 1e-3\nLR_MIN = 1e-7\nLR_RAMPUP_EPOCHS = 2\nLR_SUSTAIN_EPOCHS = 2\nEPOCHS = 20\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index / decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    \n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10,4))\nplt.plot(rng, lr_y, '-o')\nplt.xlabel('Epoch'); plt.ylabel('LR')\nprint('Learning Rate schedule : {:.3g} to {:3g} to {:3g}' . \\\n        format(lr_y[0], max(lr_y), lr_y[-1]))\nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nclass CFG:\n    nn = True\n    lgb = True\n    ens_weights = {'nn': 0.5, 'lgb': 0.5}\n    epochs = 10\n    batch_size = 512\n    valid_size = 5e-2\n    LR_Scheduler = []  # [LR]\n    optimizer = AdamW(learning_rate=1e-3, weight_decay=9e-7)\n     \nclass Model:\n    def __init__(self):\n        self.conf = ModelConfig(auto_imputation=True,\n                                auto_discrete=True,\n                                auto_discard_unique=True,\n                                categorical_columns='auto',\n                                fixed_embedding_dim=False,\n                                embeddings_output_dim=4,\n                                embedding_dropout=0.3,\n                                nets=['dnn_nets'],\n                                dnn_params={\n                                    'hidden_units': ((512, 0.3, True),\n                                                     (256, 0.3, True)),\n                                    'dnn_activation': 'relu',\n                                },\n                                stacking_op='add',\n                                output_use_bias=False,\n                                optimizer=CFG.optimizer,\n                                task='regression',\n                                loss='MeanAbsoluteError',\n                                metrics='MeanAbsoluteError',\n                                earlystopping_patience=1,\n                                )\n        \n        self.lgb_params = {\"n_estimators\": 2500,\n                           \"learning_rate\": 0.06,\n                           \"max_depth\": 16,\n                           \"num_leaves\": 500,\n                           \"reg_alpha\": 3.5,\n                           \"reg_lambda\": 1.5,\n                           \"colsample_bytree\": 0.9,\n                           \"colsample_bynode\": 0.6,\n                           \"min_child_samples\": 50,\n                           \"random_state\": 0,\n                           \"objective\": \"regression_l1\",\n                           \"device\": \"gpu\",\n                           \"n_jobs\": 4,\n                           \"verbose\": -1,\n                           }\n        \n        self.nn_model_consumption = DeepTable(config=self.conf)  \n        self.nn_model_production = DeepTable(config=self.conf)\n        \n        self.lgb_model_consumption = lgb.LGBMRegressor(**self.lgb_params)\n        self.lgb_model_production = lgb.LGBMRegressor(**self.lgb_params)\n\n    def fit(self, df_train_features):\n        print('nn = '+str(CFG.nn))\n        print('lgb = '+str(CFG.lgb))\n        \n        if CFG.nn == True:\n            \n            print('\\n',\"nn model consumption training.\",'\\n')\n            mask = df_train_features[\"is_consumption\"] == 1\n            self.nn_model_consumption.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n                validation_split=CFG.valid_size, shuffle=False,\n                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n                callbacks=CFG.LR_Scheduler\n            )\n        \n            # Avoid saving error\n            with K.name_scope(CFG.optimizer.__class__.__name__):\n                for i, var in enumerate(CFG.optimizer.weights):\n                    name = 'variable{}'.format(i)\n                    CFG.optimizer.weights[i] = tf.Variable(var, name=name)\n            self.conf = self.conf._replace(optimizer=CFG.optimizer)   \n            self.nn_model_production = DeepTable(config=self.conf)\n            \n            print('\\n',\"nn model production training.\",'\\n')\n            mask = df_train_features[\"is_consumption\"] == 0\n            self.nn_model_production.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n                validation_split=CFG.valid_size, shuffle=False,\n                batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2,\n                callbacks=CFG.LR_Scheduler\n            )\n        \n        if CFG.lgb == True:\n            \n            print('\\n',\"lgb model consumption training.\")\n            mask = df_train_features[\"is_consumption\"] == 1\n            self.lgb_model_consumption.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n            )\n        \n            print('\\n',\"lgb model production training.\",'\\n')\n            mask = df_train_features[\"is_consumption\"] == 0\n            self.lgb_model_production.fit(\n                X=df_train_features[mask].drop(columns=[\"target\"]),\n                y=df_train_features[mask][\"target\"]\n                - df_train_features[mask][\"target_48h\"].fillna(0),\n            )\n        \n    def plot_nn_model(self):\n        if CFG.nn == True:\n            return plot_model(self.nn_model_consumption.get_model().model)    \n\n    def predict(self, df_features):\n        predictions = np.zeros(len(df_features))\n        \n        if CFG.nn == True and CFG.lgb == True:\n            \n            print('\\n',\"nn & lgb model consumption prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 1\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + CFG.ens_weights['nn'] * (self.nn_model_consumption.predict(df_features[mask])[:,0])\n                + CFG.ens_weights['lgb'] * (self.lgb_model_consumption.predict(df_features[mask])),\n                0,\n                np.inf,\n            )\n        \n            print('\\n',\"nn & lgb model production prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 0\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + CFG.ens_weights['nn'] * (self.nn_model_production.predict(df_features[mask])[:,0])\n                + CFG.ens_weights['lgb'] * (self.lgb_model_production.predict(df_features[mask])),\n                0,\n                np.inf,\n            )\n        \n        elif CFG.nn == True and CFG.lgb == False:\n            \n            print('\\n',\"nn model consumption prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 1\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.nn_model_consumption.predict(df_features[mask])[:,0],\n                0,\n                np.inf,\n            )\n            \n            print('\\n',\"nn model production prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 0\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.nn_model_production.predict(df_features[mask])[:,0],\n                0,\n                np.inf,\n            )\n            \n        elif CFG.nn == False and CFG.lgb == True:\n            \n            print('\\n',\"lgb model consumption prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 1\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.lgb_model_consumption.predict(df_features[mask]),\n                0,\n                np.inf,\n            )\n            \n            print('\\n',\"lgb model production prediction.\",'\\n')\n            mask = df_features[\"is_consumption\"] == 0\n            predictions[mask.values] = np.clip(\n                df_features[mask][\"target_48h\"].fillna(0).values\n                + self.lgb_model_production.predict(df_features[mask]),\n                0,\n                np.inf,\n            )\n            \n        else:\n            raise ValueError(\"No models has been trained.\")\n            \n        return predictions\n    \n    \nmodel = Model()\nmodel.fit(df_train_features)\n\njoblib.dump(model.lgb_model_consumption, 'lgb_model_consumption.joblib')\njoblib.dump(model.lgb_model_production, 'lgb_model_production.joblib')\n\nnn_model_consumption = model.nn_model_consumption.get_model().model\nnn_model_consumption.save('nn_model_consumption.h5')\n\nnn_model_production = model.nn_model_production.get_model().model\nnn_model_production.save('nn_model_production.h5')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 저장되어있는 모델 사용하는 경우\n\n# lgb_model_consumption = joblib.load('lgb_model_consumption.joblib')\n# lgb_model_production = joblib.load('lgb_model_production.joblib')\n\n# from tensorflow.keras.models import load_model\n# model_file = 'nn_model_production.h5'\n# loaded_model = load_model(model_file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_memory()\nmodel.plot_nn_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit API","metadata":{}},{"cell_type":"code","source":"import enefit\n\nenv = enefit.make_env()\niter_test = env.iter_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor (\n    df_test, \n    df_new_target, \n    df_new_client, \n    df_new_historical_weather,\n    df_new_forecast_weather, \n    df_new_electricity_prices, \n    df_new_gas_prices, \n    df_sample_prediction\n) in iter_test:\n\n    data_storage.update_with_new_data(\n        df_new_client=df_new_client,\n        df_new_gas_prices=df_new_gas_prices,\n        df_new_electricity_prices=df_new_electricity_prices,\n        df_new_forecast_weather=df_new_forecast_weather,\n        df_new_historical_weather=df_new_historical_weather,\n        df_new_target=df_new_target\n    )\n    df_test = data_storage.preprocess_test(df_test)\n    df_test = data_storage.run(df_test)\n    \n    df_test = features_generator.generate_features(df_test)\n    df_test = df_test[df_test['target'].notnull()]\n    df_test = add_custom_features(df_test)\n\n    #이거 먼저 와야함, 예림 추가\n    df_test['eic_count'] = df_test['eic_count'].fillna(method = 'bfill')\n    df_test['installed_capacity'] = df_test['installed_capacity'].fillna(method = 'bfill')\n    #add yelim\n    df_test = DataTransformer(df_test)\n    df_test = df_test.transform()\n    #add joonyong\n    df_test = TrainDataTransform(df_test)\n    df_test = df_test.transform()\n    ### drop county == 12 , prec_change Nan,  yelim added ####\n    df_test = df_test[df_test['county'] != 12]\n    df_test_features = df_test['precipitation_change'].fillna(method='bfill')\n    \n    prec = model.predict(df_test_features)\n    \n    \n    df_sample_prediction[\"target\"] = prec\n    env.predict(df_sample_prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prec[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/example_test_files/test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/train.csv')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}